{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from typing import List, Tuple\n",
    "from torchvision import io\n",
    "from torchvision import transforms\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inchannels,\n",
    "        outchannels,\n",
    "        kernel_size: int=3,\n",
    "        prev_kernel_size: int=3,\n",
    "        prev_n_channels: int=64):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self._kernel_size = kernel_size\n",
    "        self._prev_kernel_size = prev_kernel_size\n",
    "        self._prev_n_channels = prev_n_channels\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            inchannels, \n",
    "            outchannels,\n",
    "            kernel_size=self._kernel_size\n",
    "            )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            outchannels,\n",
    "            outchannels,\n",
    "            kernel_size=self._kernel_size\n",
    "            )\n",
    "        self.net = nn.Sequential(\n",
    "            self.conv1, \n",
    "            nn.ReLU(),\n",
    "            self.conv2,\n",
    "            nn.ReLU())\n",
    "    \n",
    "    def initialize_params(self):\n",
    "        for layer in self.named_layers():\n",
    "            print(layer, torch.sqrt(2./(self._prev_kernel_size**2) * self._prev_n_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, inchannels: int=1, channel_list: Tuple=(1,64,128,256,512,1024,)):\n",
    "        super(UNet, self).__init__()\n",
    "        self.conv1 = ConvLayer(channel_list[0], channel_list[1],3,1,1)\n",
    "        self.pool1 = nn.MaxPool2d((2,2), 2)\n",
    "        self.conv2 = ConvLayer(\n",
    "            channel_list[1], \n",
    "            channel_list[2], \n",
    "            prev_kernel_size=3, \n",
    "            prev_n_channels=channel_list[1])\n",
    "        self.pool2 = nn.MaxPool2d((2,2), 2)\n",
    "        self.conv3 = ConvLayer(\n",
    "            channel_list[2], \n",
    "            channel_list[3],\n",
    "            prev_kernel_size=3,\n",
    "            prev_n_channels=channel_list[2]\n",
    "            )\n",
    "        self.pool3 = nn.MaxPool2d((2,2), 2)\n",
    "        self.conv4 = ConvLayer(\n",
    "            channel_list[3], \n",
    "            channel_list[4],\n",
    "            prev_kernel_size=3,\n",
    "            prev_n_channels=channel_list[3]\n",
    "            )\n",
    "        self.pool4 = nn.MaxPool2d((2,2), 2)\n",
    "        self.conv5 = ConvLayer(\n",
    "            channel_list[4], \n",
    "            channel_list[5],\n",
    "            prev_kernel_size=3,\n",
    "            prev_n_channels=channel_list[4]\n",
    "            )        \n",
    "\n",
    "        self.upsample1 = nn.ConvTranspose2d(channel_list[5], channel_list[4], 2, 2)\n",
    "        self.conv6 = ConvLayer(\n",
    "            channel_list[5],\n",
    "            channel_list[4],\n",
    "            prev_kernel_size=3,\n",
    "            prev_n_channels=channel_list[4]\n",
    "            )\n",
    "        self.upsample2 = nn.ConvTranspose2d(channel_list[4], channel_list[3], 2,2)\n",
    "        \n",
    "        self.conv7 = ConvLayer(\n",
    "            channel_list[4],\n",
    "            channel_list[3],\n",
    "            prev_kernel_size=3,\n",
    "            prev_n_channels=channel_list[3]\n",
    "            )\n",
    "        \n",
    "        self.upsample3 = nn.ConvTranspose2d(channel_list[3], channel_list[2], 2,2)\n",
    "        self.conv8 = ConvLayer(\n",
    "            channel_list[3],\n",
    "            channel_list[2],\n",
    "            prev_kernel_size=3,\n",
    "            prev_n_channels=channel_list[2]\n",
    "            )\n",
    "        self.upsample4 = nn.ConvTranspose2d(channel_list[2], channel_list[1], 2,2)\n",
    "        self.conv9 = ConvLayer(\n",
    "            channel_list[2],\n",
    "            channel_list[1],\n",
    "            prev_kernel_size=3,\n",
    "            prev_n_channels=channel_list[1]\n",
    "            )\n",
    "        self.conv10 = nn.Conv2d(channel_list[1], 2, 1, 1)\n",
    "        self.activation = nn.Softmax(dim=1)\n",
    "\n",
    "    def initialize_params(self):\n",
    "        for name, layer in self.named_parameters():\n",
    "            if \"conv\" in name:\n",
    "                layer.initialize_params()\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(self.pool1(x1))\n",
    "        x3 = self.conv3(self.pool2(x2))\n",
    "        x4 = self.conv4(self.pool3(x3))\n",
    "        x5 = self.conv5(self.pool4(x4))\n",
    "        \n",
    "        x_up_1 = self.upsample1(x5)\n",
    "        x_up_1 = torch.concat((\n",
    "            transforms.CenterCrop(x_up_1.shape[2:3])(x4),\n",
    "            x_up_1), dim=1)\n",
    "\n",
    "        x_up_2 = self.upsample2(self.conv6(x_up_1))\n",
    "        x_up_2 = torch.concat(\n",
    "            (transforms.CenterCrop(x_up_2.shape[2:3])(x3),\n",
    "            x_up_2), dim=1)\n",
    "\n",
    "        x_up_3 = self.upsample3(self.conv7(x_up_2))\n",
    "        x_up_3 = torch.concat(\n",
    "            (transforms.CenterCrop(x_up_3.shape[2:3])(x2),\n",
    "            x_up_3), dim=1)\n",
    "\n",
    "        x_up_4 = self.upsample4(self.conv8(x_up_3))\n",
    "        x_up_4 = torch.concat(\n",
    "            (transforms.CenterCrop(x_up_4.shape[2:3])(x1),\n",
    "            x_up_4), dim=1)\n",
    "\n",
    "        xout = self.conv10(self.conv9(x_up_4))\n",
    "        return self.activation(xout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1024, 1024])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the output shape of UNet\n",
    "test_im = io.read_image('../xray_samp/sample/images/00000013_005.png')\n",
    "test_im = test_im.float().unsqueeze(0)\n",
    "test_im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('conv1.conv1.weight', Parameter containing:\n",
      "tensor([[[[-0.2236,  0.1261, -0.0761],\n",
      "          [ 0.0939,  0.2668,  0.1707],\n",
      "          [ 0.0949,  0.0703,  0.2829]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1893,  0.2719,  0.1629],\n",
      "          [ 0.2106,  0.2429, -0.1048],\n",
      "          [ 0.2301, -0.0748, -0.0665]]],\n",
      "\n",
      "\n",
      "        [[[-0.2061,  0.3115, -0.1006],\n",
      "          [ 0.1470, -0.0535,  0.0560],\n",
      "          [-0.0963, -0.2960,  0.1497]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2358, -0.0700,  0.1266],\n",
      "          [-0.0021, -0.2337,  0.3283],\n",
      "          [-0.0972, -0.2653, -0.0564]]],\n",
      "\n",
      "\n",
      "        [[[-0.0634,  0.0131,  0.3323],\n",
      "          [ 0.3140, -0.0408,  0.3287],\n",
      "          [-0.0625,  0.0909,  0.2788]]],\n",
      "\n",
      "\n",
      "        [[[-0.0443,  0.3160, -0.2832],\n",
      "          [ 0.1095, -0.1287,  0.2741],\n",
      "          [-0.2002,  0.1846, -0.1373]]],\n",
      "\n",
      "\n",
      "        [[[-0.2954, -0.2281,  0.1123],\n",
      "          [ 0.0411,  0.0865, -0.2304],\n",
      "          [ 0.2855, -0.1077, -0.1207]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3038,  0.2378, -0.1094],\n",
      "          [ 0.1660, -0.1099, -0.1408],\n",
      "          [ 0.0223, -0.2931, -0.2591]]],\n",
      "\n",
      "\n",
      "        [[[-0.3192, -0.1894, -0.0385],\n",
      "          [-0.1628,  0.3096, -0.2415],\n",
      "          [-0.1170, -0.1228, -0.2693]]],\n",
      "\n",
      "\n",
      "        [[[-0.0212, -0.2379,  0.1556],\n",
      "          [-0.2015, -0.1455,  0.0608],\n",
      "          [-0.3205,  0.2391, -0.2262]]],\n",
      "\n",
      "\n",
      "        [[[-0.1766, -0.3333, -0.3076],\n",
      "          [-0.0023, -0.0670, -0.1834],\n",
      "          [-0.0400, -0.3091,  0.0038]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0988, -0.2263,  0.0402],\n",
      "          [-0.2875,  0.1762,  0.2779],\n",
      "          [ 0.0876, -0.2293,  0.0498]]],\n",
      "\n",
      "\n",
      "        [[[-0.1035, -0.1699, -0.0282],\n",
      "          [-0.2280,  0.1697, -0.0291],\n",
      "          [ 0.2982,  0.2839, -0.2355]]],\n",
      "\n",
      "\n",
      "        [[[-0.0251, -0.2872,  0.0126],\n",
      "          [ 0.3105, -0.0061, -0.2559],\n",
      "          [ 0.2907, -0.0024,  0.1186]]],\n",
      "\n",
      "\n",
      "        [[[-0.0721, -0.1534,  0.2518],\n",
      "          [-0.2171, -0.1695,  0.2516],\n",
      "          [-0.1455,  0.1762,  0.2745]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1713, -0.3111, -0.1658],\n",
      "          [ 0.3319, -0.2258,  0.3080],\n",
      "          [-0.3331, -0.2161,  0.1890]]],\n",
      "\n",
      "\n",
      "        [[[-0.0378, -0.0126,  0.3324],\n",
      "          [ 0.0116, -0.2037, -0.0074],\n",
      "          [ 0.1806, -0.0707,  0.3035]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2986, -0.0968,  0.0992],\n",
      "          [ 0.0836, -0.0514, -0.2591],\n",
      "          [-0.0511,  0.3177,  0.2810]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0780, -0.2260, -0.0525],\n",
      "          [-0.1868,  0.1726, -0.0224],\n",
      "          [ 0.2430,  0.1168,  0.1836]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0751, -0.0274,  0.2519],\n",
      "          [ 0.0263, -0.2856, -0.1610],\n",
      "          [ 0.1810, -0.0568, -0.1842]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0705, -0.2592, -0.2997],\n",
      "          [-0.2868, -0.3223,  0.2698],\n",
      "          [-0.0659,  0.1647,  0.1829]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0132,  0.0505, -0.1102],\n",
      "          [-0.2942,  0.1356,  0.2547],\n",
      "          [-0.1155, -0.2060, -0.0550]]],\n",
      "\n",
      "\n",
      "        [[[-0.2043,  0.1371, -0.3330],\n",
      "          [-0.1347,  0.2566,  0.0623],\n",
      "          [ 0.0925, -0.1872,  0.0820]]],\n",
      "\n",
      "\n",
      "        [[[-0.1674, -0.0500, -0.1109],\n",
      "          [-0.3132, -0.1660, -0.2945],\n",
      "          [-0.1112, -0.1298,  0.1558]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1824,  0.2898,  0.1572],\n",
      "          [ 0.1513,  0.2449, -0.0632],\n",
      "          [ 0.2779, -0.0146, -0.1359]]],\n",
      "\n",
      "\n",
      "        [[[-0.1311,  0.0161,  0.0011],\n",
      "          [-0.1131, -0.1124,  0.0064],\n",
      "          [ 0.0209, -0.2085,  0.2856]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2965, -0.2069,  0.0325],\n",
      "          [ 0.0805,  0.2755, -0.2060],\n",
      "          [-0.1501,  0.1982,  0.2796]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1824,  0.0650,  0.2756],\n",
      "          [ 0.0041,  0.0243,  0.1938],\n",
      "          [-0.3316,  0.2478, -0.0313]]],\n",
      "\n",
      "\n",
      "        [[[-0.0179,  0.1140, -0.1255],\n",
      "          [-0.0975,  0.0783,  0.1893],\n",
      "          [ 0.2538,  0.1985,  0.2299]]],\n",
      "\n",
      "\n",
      "        [[[-0.3188,  0.2483, -0.2770],\n",
      "          [-0.1983, -0.0573,  0.2897],\n",
      "          [ 0.3195, -0.1640, -0.0599]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2413,  0.1029, -0.0952],\n",
      "          [-0.1813,  0.2608, -0.0039],\n",
      "          [ 0.1957,  0.0465,  0.2793]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2753,  0.1965, -0.1360],\n",
      "          [-0.2177, -0.3223, -0.0239],\n",
      "          [-0.3136,  0.2965,  0.3221]]],\n",
      "\n",
      "\n",
      "        [[[-0.0636, -0.2930, -0.0202],\n",
      "          [-0.1471, -0.2716,  0.0429],\n",
      "          [ 0.1984,  0.0731, -0.1814]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2773,  0.2102,  0.1839],\n",
      "          [-0.1461,  0.0442, -0.0123],\n",
      "          [ 0.1055,  0.1130, -0.0810]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2102, -0.2194,  0.1867],\n",
      "          [ 0.0295, -0.1849, -0.2017],\n",
      "          [-0.3269,  0.0067,  0.0061]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1804, -0.3109, -0.0830],\n",
      "          [ 0.2432,  0.2305, -0.1038],\n",
      "          [ 0.1538,  0.0198,  0.3253]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0926, -0.3211,  0.1874],\n",
      "          [ 0.2344, -0.1926,  0.0954],\n",
      "          [-0.1719,  0.3122,  0.3167]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0467, -0.2945,  0.3118],\n",
      "          [ 0.2824, -0.0846,  0.0345],\n",
      "          [-0.2908,  0.1141,  0.2871]]],\n",
      "\n",
      "\n",
      "        [[[-0.0937, -0.3317, -0.1030],\n",
      "          [ 0.2367, -0.1733,  0.2006],\n",
      "          [ 0.2674,  0.2105,  0.2809]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1199,  0.0566, -0.0478],\n",
      "          [ 0.0805, -0.0519,  0.1767],\n",
      "          [ 0.3159,  0.2666, -0.1500]]],\n",
      "\n",
      "\n",
      "        [[[-0.1067,  0.1631,  0.2054],\n",
      "          [ 0.2049, -0.1257, -0.3225],\n",
      "          [-0.1575,  0.0019, -0.2301]]],\n",
      "\n",
      "\n",
      "        [[[-0.1202,  0.0969, -0.1278],\n",
      "          [-0.2190, -0.0894,  0.2575],\n",
      "          [ 0.2645, -0.0065,  0.3195]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0121, -0.2627,  0.1835],\n",
      "          [ 0.0077,  0.1220, -0.0336],\n",
      "          [-0.0971, -0.0979, -0.0523]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0625,  0.3177,  0.1341],\n",
      "          [-0.2394, -0.2325, -0.0116],\n",
      "          [ 0.0386,  0.1623, -0.2130]]],\n",
      "\n",
      "\n",
      "        [[[-0.0410, -0.3185, -0.2796],\n",
      "          [ 0.1511, -0.0103, -0.0978],\n",
      "          [-0.1748,  0.1701, -0.2875]]],\n",
      "\n",
      "\n",
      "        [[[-0.1478,  0.1404, -0.2854],\n",
      "          [ 0.2247, -0.0608, -0.0184],\n",
      "          [ 0.0744, -0.0869, -0.1475]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1596, -0.0637,  0.0609],\n",
      "          [ 0.0455, -0.2226, -0.2689],\n",
      "          [-0.2837,  0.2660,  0.1866]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2219, -0.3230,  0.1837],\n",
      "          [ 0.1953, -0.2690, -0.2174],\n",
      "          [ 0.2741, -0.3116, -0.2569]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2816, -0.2424, -0.0606],\n",
      "          [-0.1019,  0.2112, -0.2533],\n",
      "          [ 0.0398, -0.2176,  0.0976]]],\n",
      "\n",
      "\n",
      "        [[[-0.2538, -0.1089,  0.0706],\n",
      "          [ 0.2392,  0.0473, -0.0566],\n",
      "          [ 0.0137,  0.0364, -0.0032]]],\n",
      "\n",
      "\n",
      "        [[[-0.3301, -0.1224,  0.0993],\n",
      "          [-0.1116,  0.2283,  0.0215],\n",
      "          [ 0.2586,  0.0107,  0.1846]]],\n",
      "\n",
      "\n",
      "        [[[-0.1177, -0.1223, -0.2662],\n",
      "          [-0.2197,  0.1520,  0.0184],\n",
      "          [-0.1249, -0.0260, -0.1414]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2189, -0.3239, -0.2045],\n",
      "          [ 0.2469, -0.0157,  0.0997],\n",
      "          [-0.0711, -0.1249,  0.1821]]],\n",
      "\n",
      "\n",
      "        [[[-0.2193,  0.0418,  0.3278],\n",
      "          [-0.0720, -0.2886, -0.0319],\n",
      "          [-0.1627, -0.2052,  0.1737]]],\n",
      "\n",
      "\n",
      "        [[[-0.1154, -0.2483,  0.1356],\n",
      "          [-0.1793, -0.2685,  0.0358],\n",
      "          [ 0.1540,  0.2297,  0.1821]]],\n",
      "\n",
      "\n",
      "        [[[-0.1480,  0.1974,  0.0699],\n",
      "          [-0.2350,  0.1096,  0.0638],\n",
      "          [-0.0525,  0.1550, -0.0148]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0631, -0.2036, -0.0440],\n",
      "          [-0.3180, -0.1544, -0.2190],\n",
      "          [ 0.1300, -0.3026,  0.2124]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1051, -0.2908,  0.1820],\n",
      "          [-0.1449, -0.0543, -0.1657],\n",
      "          [ 0.2694,  0.1034, -0.0606]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1268, -0.2083,  0.0622],\n",
      "          [-0.3150, -0.0587,  0.0762],\n",
      "          [ 0.2096, -0.1431,  0.1088]]],\n",
      "\n",
      "\n",
      "        [[[-0.0560, -0.3234, -0.3285],\n",
      "          [ 0.0382, -0.2001, -0.2506],\n",
      "          [ 0.2856, -0.2606,  0.1421]]],\n",
      "\n",
      "\n",
      "        [[[-0.2020,  0.0317,  0.1417],\n",
      "          [-0.1383, -0.2965,  0.0122],\n",
      "          [ 0.2535,  0.0343, -0.1410]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2538,  0.0023, -0.1488],\n",
      "          [ 0.0989, -0.2475,  0.2095],\n",
      "          [-0.1006, -0.3103,  0.2092]]],\n",
      "\n",
      "\n",
      "        [[[-0.1535,  0.0116, -0.2042],\n",
      "          [-0.1682,  0.3145, -0.2875],\n",
      "          [ 0.2642,  0.2974,  0.2767]]],\n",
      "\n",
      "\n",
      "        [[[-0.0900, -0.1911, -0.2552],\n",
      "          [-0.0327, -0.1057, -0.0370],\n",
      "          [-0.1707, -0.0446,  0.0404]]]], requires_grad=True))\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w5/2v0w9g6936v76hvg244v5gxw0000gp/T/ipykernel_49982/3044756030.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_im\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/w5/2v0w9g6936v76hvg244v5gxw0000gp/T/ipykernel_49982/3442432165.py\u001b[0m in \u001b[0;36minitialize_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = UNet(1)\n",
    "net.initialize_params()\n",
    "out = net(test_im)\n",
    "out.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Initialize network weights as follows :  \"For a network with our architecture (alternating convolution and ReLU layers) this can be achievedby  drawing  the  initial  weights  from  a  Gaussian  distribution  with  a  standard deviation of √2/N, where N denotes the number of incoming nodes of one neuron [5]. E.g. for a 3x3 convolution and 64 feature channels in the previous layer N= 9·64 = 576\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "only convolutional and upsampleolutional layers have learnable parameters.\n",
    "So, search for each convolutional layer, to initialize, \n",
    "simply ask what the previous convolutional layer's kernel size was,\n",
    "and find the number of channels of that previous layer (dim[1])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1._conv1.weight\n",
      "conv1._conv1.bias\n",
      "conv1._conv2.weight\n",
      "conv1._conv2.bias\n",
      "conv2._conv1.weight\n",
      "conv2._conv1.bias\n",
      "conv2._conv2.weight\n",
      "conv2._conv2.bias\n",
      "conv3._conv1.weight\n",
      "conv3._conv1.bias\n",
      "conv3._conv2.weight\n",
      "conv3._conv2.bias\n",
      "conv4._conv1.weight\n",
      "conv4._conv1.bias\n",
      "conv4._conv2.weight\n",
      "conv4._conv2.bias\n",
      "conv5._conv1.weight\n",
      "conv5._conv1.bias\n",
      "conv5._conv2.weight\n",
      "conv5._conv2.bias\n",
      "deconv1.weight\n",
      "deconv1.bias\n",
      "conv6._conv1.weight\n",
      "conv6._conv1.bias\n",
      "conv6._conv2.weight\n",
      "conv6._conv2.bias\n",
      "deconv2.weight\n",
      "deconv2.bias\n",
      "conv7._conv1.weight\n",
      "conv7._conv1.bias\n",
      "conv7._conv2.weight\n",
      "conv7._conv2.bias\n",
      "deconv3.weight\n",
      "deconv3.bias\n",
      "conv8._conv1.weight\n",
      "conv8._conv1.bias\n",
      "conv8._conv2.weight\n",
      "conv8._conv2.bias\n",
      "deconv4.weight\n",
      "deconv4.bias\n",
      "conv9._conv1.weight\n",
      "conv9._conv1.bias\n",
      "conv9._conv2.weight\n",
      "conv9._conv2.bias\n",
      "conv10.weight\n",
      "conv10.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('unet_prax-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bba0aa6026272d72fa4312733ee44244b566d0cb10b9a208c00528c53cf61bbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
